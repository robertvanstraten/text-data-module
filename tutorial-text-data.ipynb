{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial (Text Data Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Last updated: Mar 3, 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will familiarize you with the data science pipeline of processing text data. We will go through the various steps involved in the NLP pipeline for topic modelling and topic classification, including tokenization, lemmatization, and obtaining word embeddings. We will also build a neural network using PyTorch for multi-class topic classification using the dataset.\n",
    "The AG's News Topic Classification Dataset contains news articles from four different categories, making it a nice source of text data for NLP tasks. We will guide you through the process of understanding the dataset, implementing various NLP techniques, and building a model for classification. Below is the pipeline of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pipeline pic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following link to jump to the tasks and assignments:\n",
    "\n",
    "[table of contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [AG's News Topic Classification Dataset](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv) is a collection of over 1 million news articles from more than 2000 news sources. The dataset was created by selecting the 4 largest classes from the original corpus, resulting in 120,000 training samples and 7,600 testing samples. The dataset is provided by the academic community for research purposes in data mining, information retrieval, and other non-commercial activities. We will use it to demonstrate various NLP techniques on real data, and in the end make 2 models with this data. The files train.csv and test.csv contain all the training and testing samples as comma-separated values with 3 columns: class index, title, and description. Download train.csv and test.csv for the following tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put all the packages that are needed for this tutorial below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below contains answers for the assignments in this tutorial. **Do not check the answers in the next cell before practicing the tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_df(df_result, df_answer, n=1):\n",
    "    \"\"\"\n",
    "    This function checks if two output dataframes are the same.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_result : pandas.DataFrame\n",
    "        The result from the output of a function.\n",
    "    df_answer: pandas.DataFrame\n",
    "        The expected output of the function.\n",
    "    n : int\n",
    "        The numbering of the test case.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df_answer.isinstance(list):\n",
    "            assert any([answer.equals(df_result) for answer in df_answer])\n",
    "        else:\n",
    "            assert df_answer.equals(df_result)\n",
    "        print(f\"Test case {n} passed.\")\n",
    "    except:\n",
    "        print(f\"Test case {n} failed.\")\n",
    "        print(\"\")\n",
    "        print(\"Your output is:\")\n",
    "        print(df_result)\n",
    "        print(\"\")\n",
    "        print(\"Expected output is\", end=\"\")\n",
    "        if df_answer.isinstance(list):\n",
    "            print(\" one of\", end=\"\")\n",
    "        print(\":\")\n",
    "        print(df_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Preprocess Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will preprocess the text data from the AG News Dataset. First, we need to load the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>KARACHI (Reuters) - Pakistani President Perve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Renteria signing a top-shelf deal</td>\n",
       "      <td>Red Sox general manager Theo Epstein acknowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Saban not going to Dolphins yet</td>\n",
       "      <td>The Miami Dolphins will put their courtship of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Today's NFL games</td>\n",
       "      <td>PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Nets get Carter from Raptors</td>\n",
       "      <td>INDIANAPOLIS -- All-Star Vince Carter was trad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class Index                                              Title  \\\n",
       "0                 3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1                 3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2                 3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3                 3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4                 3  Oil prices soar to all-time record, posing new...   \n",
       "...             ...                                                ...   \n",
       "119995            1  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996            2                  Renteria signing a top-shelf deal   \n",
       "119997            2                    Saban not going to Dolphins yet   \n",
       "119998            2                                  Today's NFL games   \n",
       "119999            2                       Nets get Carter from Raptors   \n",
       "\n",
       "                                              Description  \n",
       "0       Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1       Reuters - Private investment firm Carlyle Grou...  \n",
       "2       Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3       Reuters - Authorities have halted oil export\\f...  \n",
       "4       AFP - Tearaway world oil prices, toppling reco...  \n",
       "...                                                   ...  \n",
       "119995   KARACHI (Reuters) - Pakistani President Perve...  \n",
       "119996  Red Sox general manager Theo Epstein acknowled...  \n",
       "119997  The Miami Dolphins will put their courtship of...  \n",
       "119998  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...  \n",
       "119999  INDIANAPOLIS -- All-Star Vince Carter was trad...  \n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>1</td>\n",
       "      <td>Around the world</td>\n",
       "      <td>Ukrainian presidential candidate Viktor Yushch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>2</td>\n",
       "      <td>Void is filled with Clement</td>\n",
       "      <td>With the supply of attractive pitching options...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>2</td>\n",
       "      <td>Martinez leaves bitter</td>\n",
       "      <td>Like Roger Clemens did almost exactly eight ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>3</td>\n",
       "      <td>5 of arthritis patients in Singapore take Bext...</td>\n",
       "      <td>SINGAPORE : Doctors in the United States have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>3</td>\n",
       "      <td>EBay gets into rentals</td>\n",
       "      <td>EBay plans to buy the apartment and home renta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class Index                                              Title  \\\n",
       "0               3                  Fears for T N pension after talks   \n",
       "1               4  The Race is On: Second Private Team Sets Launc...   \n",
       "2               4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3               4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4               4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "...           ...                                                ...   \n",
       "7595            1                                   Around the world   \n",
       "7596            2                        Void is filled with Clement   \n",
       "7597            2                             Martinez leaves bitter   \n",
       "7598            3  5 of arthritis patients in Singapore take Bext...   \n",
       "7599            3                             EBay gets into rentals   \n",
       "\n",
       "                                            Description  \n",
       "0     Unions representing workers at Turner   Newall...  \n",
       "1     SPACE.com - TORONTO, Canada -- A second\\team o...  \n",
       "2     AP - A company founded by a chemistry research...  \n",
       "3     AP - It's barely dawn when Mike Fitzpatrick st...  \n",
       "4     AP - Southern California's smog-fighting agenc...  \n",
       "...                                                 ...  \n",
       "7595  Ukrainian presidential candidate Viktor Yushch...  \n",
       "7596  With the supply of attractive pitching options...  \n",
       "7597  Like Roger Clemens did almost exactly eight ye...  \n",
       "7598  SINGAPORE : Doctors in the United States have ...  \n",
       "7599  EBay plans to buy the apartment and home renta...  \n",
       "\n",
       "[7600 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "display(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text into individual tokens, which are usually words but can also be phrases or sentences. While it may seem like a trivial task, tokenization can be applied in multiple ways and thus be a complex and challenging task influencing natural language processing (NLP) applications. This is because different languages and even different contexts within the same language can have vastly different tokenization rules.\n",
    "\n",
    "For example, in languages like English and Dutch, it is generally straightforward to identify words by using spaces as delimiters. However, there are exceptions, such as contractions like \"can't\" and hyphenated words like \"self-driving\". In other languages, such as Chinese and Japanese, there are no spaces between words, so identifying word boundaries is much more difficult.\n",
    "\n",
    "Moreover, tokenization is often a crucial step in the NLP pipeline because the accuracy of the subsequent analysis depends on the quality of the tokens. Poor tokenization can lead to inaccurate results and can make it difficult to extract meaningful information from the text.\n",
    "\n",
    "To illustrate the importance of tokenization, let's consider an example in Python using the NLTK library. The following code tokenizes a sample text using the `word_tokenize` function from the NLTK package, which uses a pre-trained tokenization model for English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The quick brown fox jumped over the lazy dog. The dog couldn't wait to sleep all day.\n",
      "Tokenized text: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'could', \"n't\", 'wait', 'to', 'sleep', 'all', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"The quick brown fox jumped over the lazy dog. The dog couldn't wait to sleep all day.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokenized text:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization or stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing both, but stick to one for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the stemmer and lemmatizer\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Perform stemming and lemmatization on the tokens\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Print the results\n",
    "print(\"Stemmed text:\", stemmed_tokens)\n",
    "print(\"Lemmatized text:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assignment: Show word frequencies; top-n overall, per topic from the dataset both before and after pre-processing\n",
    "\n",
    "Sources:\n",
    "\n",
    "- https://www.kaggle.com/code/vukglisovic/classification-combining-lda-and-word2vec/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered text: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', '.', 'dog', 'sleep', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove English stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in lemmatized_tokens if token.lower() not in english_stopwords]\n",
    "print(\"Filtered text:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Another option: spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make a seperate column with the text transformed to a SpaCy span per column\n",
    "- Show that tokens, lemma's, stems and if something is a stopword is immediately available on the span.\n",
    "- Assignment: make a version of the text that's tokenized, lemmatized/stemmed and has stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The quick brown fox jumped over the lazy dog. The dog slept all day.\n",
      "Stemmed text: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', '.', 'dog', 'sleep', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumped over the lazy dog. The dog slept all day.\"\n",
    "\n",
    "# Tokenize the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Perform stemming and lemmatization on the tokens\n",
    "lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original text:\", text)\n",
    "print(\"Stemmed text:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Unsupervised Learning - Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use LDA to transform preprocessed text into features\n",
    "- Run simple Kmeans to make clusters\n",
    "- Let student pick amount of clusters (elbow method)\n",
    "- Evaluate using adjusted_mutual_info_score and adjusted_rand_score\n",
    "- Similar to an assignment from the Applied ML course these students had last year, but LDA was cut for their year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load preprocessed text data\n",
    "data = pd.read_csv('preprocessed_text.csv')\n",
    "\n",
    "# Define the number of topics to extract with LDA\n",
    "num_topics = 10\n",
    "\n",
    "# Convert preprocessed text to features using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['preprocessed_text'])\n",
    "\n",
    "# Fit LDA to the feature matrix\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=10, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Extract the topic proportions for each document\n",
    "doc_topic_proportions = lda.transform(X)\n",
    "\n",
    "# Determine the optimal number of clusters with KMeans using the elbow method\n",
    "k_range = range(2, 11)\n",
    "sse = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=100)\n",
    "    kmeans.fit(doc_topic_proportions)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot the elbow curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_range, sse)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()\n",
    "\n",
    "# Let the user select the number of clusters\n",
    "num_clusters = int(input(\"Enter the number of clusters: \"))\n",
    "\n",
    "# Cluster the documents using KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter=100)\n",
    "kmeans.fit(doc_topic_proportions)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Evaluate the clustering using adjusted mutual information score and adjusted rand score\n",
    "ami_score = adjusted_mutual_info_score(data['category'], cluster_labels)\n",
    "ari_score = adjusted_rand_score(data['category'], cluster_labels)\n",
    "\n",
    "print(f\"Adjusted mutual information score: {ami_score:.2f}\")\n",
    "print(f\"Adjusted rand score: {ari_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show code to make embeddings based on pre-processed text using both NLTK and spaCy\n",
    "- Assignment: Let student apply it to dataframe\n",
    "\n",
    "Sources:\n",
    "- https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\n",
    "- https://www.kaggle.com/code/vukglisovic/classification-combining-lda-and-word2vec/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load preprocessed text data\n",
    "data = pd.read_csv('preprocessed_text.csv')\n",
    "\n",
    "# Define the preprocessing functions\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "data['tokens'] = data['preprocessed_text'].apply(preprocess_text)\n",
    "\n",
    "# Train a Word2Vec model on the preprocessed text data\n",
    "model = Word2Vec(data['tokens'], size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the word embedding for a specific word\n",
    "embedding = model.wv['word']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.8810937e-01,  1.2690233e+00,  1.6575636e+00, -9.0438890e-01,\n",
       "       -7.0677483e-01, -1.1966441e+00,  4.2419106e-01, -4.2574078e-01,\n",
       "       -2.8125042e-01,  1.2154953e+00,  1.0961263e-01, -9.9083734e-01,\n",
       "       -8.9280665e-01, -9.2996472e-01, -1.2193933e+00, -3.3290449e-01,\n",
       "       -1.2119348e+00,  7.6204681e-01,  4.9417186e+00, -3.7561244e-01,\n",
       "        2.1576166e-02, -5.2177596e-01, -2.1905096e+00, -7.6049173e-01,\n",
       "       -1.4267705e-01,  2.4515245e+00, -2.9129535e-04,  3.4355882e-01,\n",
       "        1.1452764e+00, -1.3602724e+00, -1.2848355e+00,  3.1477764e-02,\n",
       "        7.5193155e-01,  7.0128936e-01,  2.0565279e+00,  9.5156097e-01,\n",
       "        4.5888591e-01,  1.1683748e+00,  3.1925082e-01, -9.0628773e-01,\n",
       "       -6.1355400e-01, -8.2875299e-01, -3.2473198e-01, -9.0215296e-01,\n",
       "       -4.9787417e-01, -8.8159394e-01, -8.8454676e-01, -1.1683216e+00,\n",
       "       -7.5443119e-02,  1.3703958e+00, -6.6398099e-02,  2.9801071e-01,\n",
       "        6.4264160e-01,  3.4167087e-01,  2.9193616e-01, -3.6580968e-01,\n",
       "       -1.4615867e+00,  4.8184878e-01, -2.1468832e-01, -5.3510070e-01,\n",
       "       -1.1529505e+00, -2.6276374e-01, -5.2934057e-01,  3.0656588e-01,\n",
       "       -3.7726715e-02,  1.1173865e+00,  1.5275910e-01,  1.9924023e+00,\n",
       "        1.4308605e+00,  5.6672281e-01,  3.3257836e-01, -7.0998895e-01,\n",
       "       -1.0103250e+00, -2.2918582e-02,  1.9224594e+00, -3.4831306e-01,\n",
       "        1.9020852e-01, -2.0487618e-01, -3.0081892e-01, -1.4144248e+00,\n",
       "       -8.5231441e-01,  3.5293996e-01, -1.1631105e+00,  6.7408854e-01,\n",
       "       -1.6807383e-01, -9.4455522e-01, -3.6200175e-01, -3.0185115e-01,\n",
       "       -2.3595014e-01, -2.4525467e-01, -1.2972181e+00,  5.6925941e+00,\n",
       "       -1.1455803e+00, -3.8724178e-01, -1.4301389e-02,  1.5504851e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load preprocessed text data\n",
    "# data = pd.read_csv('preprocessed_text.csv')\n",
    "\n",
    "# # Define the preprocessing function\n",
    "# def preprocess_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "#     return tokens\n",
    "\n",
    "# # Apply the preprocessing function to the text data\n",
    "# data['tokens'] = data['preprocessed_text'].apply(preprocess_text)\n",
    "\n",
    "# Get the word embedding for a specific word\n",
    "embedding = nlp('.').vector\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Supervised Learning - Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the word embeddings features, train a small neural net \n",
    "- Don't give the full torch code, only one layer to let them do something with torch\n",
    "- Hyperparameter tuning (either some hinted ones or see if Ray Tune is worth it for this task)\n",
    "- Evaluate using confusion matrix against true features\n",
    "\n",
    "Sources:\n",
    "- https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load preprocessed text data with word embeddings as features\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['topic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple neural network with one hidden layer\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = len(X_train[0])\n",
    "hidden_dim = 100\n",
    "output_dim = len(y_train.unique())\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Initialize model, optimizer and loss function\n",
    "model = Net(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(X_train)):\n",
    "        # Convert input and target to tensors\n",
    "        input_tensor = torch.tensor(X_train.iloc[i])\n",
    "        target_tensor = torch.tensor(y_train.iloc[i])\n",
    "        \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target_tensor)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X_test)):\n",
    "        # Convert input to tensor\n",
    "        input_tensor = torch.tensor(X_test.iloc[i])\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        # Get predicted class\n",
    "        _, predicted = torch.max(output.data, 0)\n",
    "        y_pred.append(predicted.item())\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
